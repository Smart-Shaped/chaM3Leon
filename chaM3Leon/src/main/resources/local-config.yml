batch:
  spark:
    app:
      name: "batch-application"
    yarn:
      am:
        cores: 1
        memory: "1024m"
        memoryOverhead: "384m"
    driver:
      memory: "1024m"
      cores: 2
      maxResultSize: "1024m"
    executor:
      memory: "3g"
      instances: 1
      cores: 2
      memoryOverhead: "1024m"
      heartbeatInterval: 60
    serializer: "org.apache.spark.serializer.KryoSerializer"
    memory:
      fraction: 0.75
      storageFraction: 0.5
    sql:
      parquet:
        compression:
          codec: "zstd"
      shuffle:
        partitions: 200
    cassandra:
      connection:
        host: "cassandra1"
        port: 9042
    hadoop:
      fs:
        defaultFS: "hdfs://namenode:8020"
      parquet:
        page:
          size:
            row:
              check:
                min: 2
                max: 10
      dfs:
        replication: 1
  kafka:
    server: "kafka1:19092"
    bootstrap-servers: kafka1:19092
    intervalMs: "30000"
    producer:
      bootstrap-servers: kafka1:19092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    topics:
      topic1:
        name: "topic1"
        class: "com.smartshaped.chameleon.batchapp.CustomPreprocessor"
        path: "/user/spark/topic1/data.parquet"
        checkpoint: "/user/spark/checkpoint/topic1"
      topic2:
        name: "topic2"
        class: "com.smartshaped.chameleon.batchapp.CustomPreprocessor"
        path: "/user/spark/topic2/data.parquet"
        checkpoint: "/user/spark/checkpoint/topic2"
  updater:
    name: "CustomUpdater"
    class: "com.smartshaped.chameleon.batchapp.CustomUpdater"
  cassandra:
    node: "cassandra1"
    port: 9042
    datacenter: "datacenter1"
    checkpoint: "/user/spark/checkpoint/cassandra"
    keyspace: 
      name: "keyspace"
    model:
      class: "com.smartshaped.chameleon.batchapp.CustomModel"
speed:
  spark:
    app:
      name: "speed-application"
    yarn:
      am:
        cores: 1
        memory: "1024m"
        memoryOverhead: "384m"
    driver:
      memory: "1024m"
      cores: 2
      maxResultSize: "1024m"
    executor:
      memory: "3g"
      instances: 1
      cores: 2
      memoryOverhead: "1024m"
      heartbeatInterval: 60
    serializer: "org.apache.spark.serializer.KryoSerializer"
    memory:
      fraction: 0.75
      storageFraction: 0.5
    sql:
      parquet:
        compression:
          codec: "zstd"
      shuffle:
        partitions: 200
    cassandra:
      connection:
        host: "cassandra1"
        port: 9042
    hadoop:
      fs:
        defaultFS: "hdfs://namenode:8020"
      parquet:
        page:
          size:
            row:
              check:
                min: 2
                max: 10
      dfs:
        replication: 1
  kafka:
    server: "kafka1:19092"
    bootstrap-servers: kafka1:19092
    intervalMs: "30000"
    producer:
      bootstrap-servers: kafka1:19092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    topics:
      topic1:
        name: "topic1"
        class: "com.smartshaped.chameleon.batchapp.CustomPreprocessor"
        path: "/user/spark/topic1/data.parquet"
        checkpoint: "/user/spark/checkpoint/topic1"
      topic2:
        name: "topic2"
        class: "com.smartshaped.chameleon.batchapp.CustomPreprocessor"
        path: "/user/spark/topic2/data.parquet"
        checkpoint: "/user/spark/checkpoint/topic2"
  updater:
    name: "CustomUpdater"
    class: "com.smartshaped.chameleon.batchapp.CustomUpdater"
  cassandra:
    node: "cassandra1"
    port: 9042
    datacenter: "datacenter1"
    checkpoint: "/user/spark/checkpoint/cassandra"
    keyspace: 
      name: "keyspace"
    model:
      class: "com.smartshaped.chameleon.batchapp.CustomModel"
ml:
  spark:
    app:
      name: "ml-application"
    yarn:
      am:
        cores: 1
        memory: "1G"
    driver:
      memory: "1G"
      cores: 1
    executor:
      memory: "2G"
      instances: 2
      cores: 2
    serializer: "org.apache.spark.serializer.KryoSerializer"
    sql:
      parquet:
        compression:
          codec: "zstd"
    hadoop:
      fs:
        defaultFS: "hdfs://namenode:8020"
      parquet:
        page:
          size:
            row:
              check:
                min: 2
                max: 10
      dfs:
        replication: 1
    cassandra:
      connection:
        host: "cassandra1"
        port: "9042"
  hdfs:
    modelDir: "/user/spark/model"
    readers:
      default: ""
      reader1:
        class: "com.smartshaped.chameleon.ml.CustomHdfsReader"
        path: "/user/spark/topic1/data.parquet"
      reader2:
        class: "com.smartshaped.chameleon.ml.CustomHdfsReader"
        path: "/user/spark/checkpoint/topic2"
  pipeline:
    class: "com.smartshaped.fesr.ml.CustomPipeline"
  modelSaver:
    class: "com.smartshaped.fesr.ml.CustomModelSaver"
  cassandra:
    model:
      class: "com.smartshaped.chameleon.ml.models.CustomModel"
    datacenter: "datacenter1"
    keyspace:
      name: "ml_keyspace"
      replication_factor: 1

